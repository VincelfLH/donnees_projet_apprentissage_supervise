---
title: "Projet - apprentissage supervisÃ©"
subtitle: "Executive Master Statistique et IA"
author: Vincent Le Flem
date: today
format: 
  pdf:
    documentclass: article
    papersize: a4
    fontsize: 11pt
    mainfont: "Times New Roman"
    geometry: margin=2.5cm

    toc: true
    toc-title: "Table des matiÃ¨res"
    number-sections: true
    lof: false
    lot: false

    fig-width: 4
    fig-height: 3
    fig-cap-location: bottom
    fig-align: center

    titlepage: true
    titlepage-color: "ffffff"
    titlepage-text-color: "000080" 
    titlepage-rule-color: "000080"
    titlepage-rule-height: 2

    include-in-header: header.tex
    keep-tex: true
---




# Fusion et structuration du jeu de donnÃ©es d'entrainement
Ã€ partir dâ€™analyses exploratoires - Ã  toutes fins utiles, le dÃ©tail est joint dans le notebook associÃ© - le jeu de donnÃ©es initial a Ã©tÃ© transformÃ© pour rendre exploitables certaines variables, comme Â« insee_code Â», trop fine en lâ€™Ã©tat, et pour optimiser la modÃ©lisation prÃ©dictive. L'objectif est de maximiser au mieux le compromis entre perte potentielle d'information et efficience statistique du futur modÃ¨le. En particulier, il a Ã©tÃ© optÃ© pour : 

1. La rÃ©duction de la cardinalitÃ© et la structuration des professions. Le jeu de donnÃ©es initial contenait une variable Â« job_42 Â» Ã  forte cardinalitÃ©, difficilement exploitable en lâ€™Ã©tat. En outre, cette variable partageait des similitudes importantes avec la variable Â« job_desc_n2 Â». Deux transformations complÃ©mentaires ont Ã©tÃ© mises en place :

   - Regroupement frÃ©quentiel (job_42_regroupe) : les professions les plus rares (â‰¤ 500 occurrences) ont Ã©tÃ© regroupÃ©es dans des catÃ©gories voisines afin de prÃ©server la stabilitÃ© statistique tout en conservant un certain niveau de dÃ©tail. Une premiÃ¨re analyse dâ€™erreurs des faux positifs a conduit Ã  ajuster Â« job_42_regroupe Â» pour mieux isoler certaines professions surreprÃ©sentÃ©es.

   - Regroupement sÃ©mantique (job_42_categorie_simplifiee) : une version Ã  7 grandes familles socioprofessionnelles, s'appuyant sur les fichiers csv complÃ©mentaires, a Ã©tÃ© construite Ã  partir des fichiers csv complÃ©mentaires, sur des critÃ¨res sociologiques et linguistiques, afin dâ€™optimiser les performances de modÃ©lisation et les temps de calcul.

L'analyse de la matrice de correspondance entre les deux regroupements tend Ã  confirmer leur complÃ©mentaritÃ© : Â« job_42_categorie_simplifiee Â» assure une bonne cohÃ©rence avec les regroupements sÃ©mantiques standards, tandis que Â« job_42_regroupe Â» conserve une finesse dâ€™analyse utile pour lâ€™interprÃ©tation des rÃ©sultats. Ces deux variables ont donc Ã©tÃ© conservÃ©es conjointement dans le jeu de donnÃ©es dâ€™entraÃ®nement.

En complÃ©ment, la variable Â« job_desc Â», peu exploitable Ã©galement en raison du nombre de modalitÃ©s, a Ã©tÃ© restructurÃ©e via un mapping Ã  trois niveaux hiÃ©rarchiques (N3, N2, N1) obtenu Ã  partir des fichiers Â« code_job_desc_map.csv Â», Â« code_job_desc_n1.csv Â» et Â« code_job_desc_n2.csv Â». Cette opÃ©ration a permis de dÃ©river les variables Â« job_desc_n1 Â» et Â« job_desc_n2 Â», offrant ainsi une lecture plus agrÃ©gÃ©e des libellÃ©s mÃ©tiers.

2. L'enrichissement gÃ©ographique et le calcul des distances.
   
La variable â€œinsee_codeâ€, trop fine en lâ€™Ã©tat, a Ã©tÃ© transformÃ©e pour gÃ©nÃ©rer des variables synthÃ©tiques exploitables : â€œRegionâ€, â€œNom_departementâ€ et â€œMunicipality_typeâ€.
Deux variables continues clÃ©s ont Ã©tÃ© construites :

   - Â« distance_job_km Â» : distance gÃ©odÃ©sique entre le lieu de rÃ©sidence actuel et le dÃ©partement dâ€™emploi (actifs) ;

   - Â« distance_former_km Â» : distance entre le lieu de rÃ©sidence actuel et lâ€™ancien lieu dâ€™emploi (retraitÃ©s).

La distribution de ces distances rÃ©vÃ¨le des profils contrastÃ©s : les distances Â« actuelles Â» sont concentrÃ©es autour de 10â€“50 km, tandis que les distances post-emploi sont souvent supÃ©rieures Ã  200 km. Une visualisation cartographique des flux, mÃªme faiblement peuplÃ©s, a mis en Ã©vidence des dynamiques nettes : mobilitÃ© post-emploi frÃ©quente, avec des pÃ´les dâ€™attractivitÃ© dans le sud et lâ€™ouest du pays.

Ces constats ont conduit Ã  intÃ©grer ces deux variables dans le jeu dâ€™entraÃ®nement, tant pour leur valeur prÃ©dictive potentielle que pour leur intÃ©rÃªt dâ€™interprÃ©tation sociogÃ©ographique. Des versions catÃ©gorielles en quatre classes ont Ã©galement Ã©tÃ© dÃ©rivÃ©es en version catÃ©gorielle (ex. : Â«â€¯< 10 kmâ€¯Â», Â«â€¯10â€“50 kmâ€¯Â», etc.), facilitant certaines visualisations et comparaisons intergroupes, notamment pour lâ€™analyse exploratoire des profils de mobilitÃ©.

En complÃ©ment, une analyse statistique et graphique des variables numÃ©riques a Ã©tÃ© rÃ©alisÃ©e afin de dÃ©tecter dâ€™Ã©ventuelles anomalies ou valeurs aberrantes. Les distributions observÃ©es (Ã¢ge, rÃ©munÃ©ration, pension, heures travaillÃ©es, distances domicile-travail) sont globalement cohÃ©rentes avec les attentes socio-Ã©conomiques du jeu de donnÃ©es. Des valeurs extrÃªmes ont Ã©tÃ© identifiÃ©es, notamment sur les variables Â« Remuneration Â» et Â« retirement_pay Â», mais elles correspondent Ã  des profils plausibles (ex. : cadres trÃ¨s rÃ©munÃ©rÃ©s). En lâ€™absence dâ€™anomalies structurelles manifestes, aucune transformation spÃ©cifique nâ€™a Ã©tÃ© jugÃ©e nÃ©cessaire, ces valeurs pouvant contenir une information utile Ã  la modÃ©lisation.

In fine, le jeu final a Ã©tÃ© consolidÃ© selon plusieurs Ã©tapes :

   - Harmonisation : uniformisation des identifiants (Unique_id) pour assurer les jointures entre les diffÃ©rents fichiers.

   - Nettoyage : suppression des variables inexploitables ou devenues redondantes (Â« insee_code Â», Â« job_desc Â», etc.).

   - Fusion : intÃ©gration des variables gÃ©ographiques dans le jeu principal.

   - GÃ©nÃ©ration : crÃ©ation de la variable Â« Statut Â» pour distinguer les actifs des retraitÃ©s, prÃ©requis pour une modÃ©lisation segmentÃ©e.



# SÃ©lection prÃ©alable des variables et choix du modÃ¨le prÃ©dictif

## SÃ©lection des variables
La sÃ©lection des variables sâ€™est appuyÃ©e, en particulier, sur une double approche analytique :

   - quantitative, via lâ€™information mutuelle, calculÃ©e sÃ©parÃ©ment pour les actifs, les retraitÃ©s et lâ€™ensemble global ;

   - qualitative, via lâ€™analyse de redondances (CramÃ©râ€™s V) et la pertinence mÃ©tier.

Les variables conservÃ©es doivent ainsi maximiser lâ€™information utile tout en limitant les chevauchements. Par exemple, le regroupement sÃ©mantique Â«â€¯job_42_regroupeâ€¯Â» a Ã©tÃ© prÃ©fÃ©rÃ© Ã  des variantes trop fines ou trop corrÃ©lÃ©es entre elles (comme Â«â€¯job_desc_n1â€¯Â» et Â«â€¯job_desc_n2â€¯Â»).

Les variables quasi constantes, comme Â«â€¯Job_categorâ€¯Â», prÃ©sente Ã  plus de 95 % dans une seule modalitÃ©, ont Ã©tÃ© Ã©cartÃ©es, tout comme certaines variables fortement redondantes (par exemple : Â«â€¯Departementâ€¯Â», trÃ¨s corrÃ©lÃ© Ã  Â«â€¯Regionâ€¯Â»).

Ces analyses, croisÃ©es Ã  diffÃ©rents tests de performance, ont conduit Ã  lâ€™exclusion des variables suivantes dans l'entraÃ®nement des modÃ¨les : Â« job_desc_n1 Â», Â« job_desc_n2 Â», Â« Activity_type Â», Â« Region Â», Â« Job_categor Â», Â« retirement_age Â» et Â« former_job_42 Â».

## Pipelines de prÃ©traitement
En vue de construire des pipelines de prÃ©traitement efficaces pour l'entraÃ®nement des modÃ¨les, une sÃ©lection diffÃ©renciÃ©e a Ã©tÃ© opÃ©rÃ©e selon le statut actif/retraitÃ©, en tenant compte de la distribution des variables, de leur pouvoir prÃ©dictif et du taux de complÃ©tion observÃ©.

Certaines variables ont ainsi Ã©tÃ© conservÃ©es uniquement dans un segment. Par exemple : Â«â€¯distance_job_kmâ€¯Â» chez les actifs, Â«â€¯retirement_ageâ€¯Â» ou Â«â€¯retirement_payâ€¯Â» chez les retraitÃ©s.
Dâ€™autres ont Ã©tÃ© retenues dans les deux modÃ¨les du fait de leur valeur transversale, comme Â«â€¯Qualificationâ€¯Â», Â«â€¯Municipality_typeâ€¯Â» ou Â«â€¯Contract_typeâ€¯Â».

Cette structure a permis la mise en place de pipelines segmentÃ©es, conÃ§ues respecter les particularitÃ©s statistiques et contextuelles (logique mÃ©tier) propres Ã  chaque sous-population.

Je ne suis pas parvenu Ã  trouver des donnÃ©es externes convaincantes. Dans ce contexte, jâ€™ai ainsi choisi dâ€™imputer les variables numÃ©riques sensibles, comme Â«â€¯Remunerationâ€¯Â» chez les actifs ou Â«â€¯retirement_payâ€¯Â» chez les retraitÃ©s, par KNN afin de prÃ©server les proximitÃ©s structurelles entre individus. Ce choix vise Ã  limiter la distorsion des distributions et Ã  prÃ©server les corrÃ©lations locales, susceptibles de porter un signal fort pour la prÃ©diction, notamment dans le cas de variables trÃ¨s discriminantes au regard de la variable cible.

Ã€ lâ€™inverse, certaines variables numÃ©riques comme les distances gÃ©ographiques ont Ã©tÃ© imputÃ©es par valeur constante (0), non pas comme une approximation, mais comme un signal explicite dâ€™absence dâ€™information pertinente - par exemple, la distance domicile-emploi pour un retraitÃ©.

Pour les variables catÃ©gorielles, une imputation par libellÃ©s explicites (Â«â€¯Non concernÃ©â€¯Â» ou Â«â€¯Inconnuâ€¯Â») a Ã©tÃ© privilÃ©giÃ©e. Ce choix permet de conserver une interprÃ©tabilitÃ© complÃ¨te aprÃ¨s encodage one-hot, sans perte d'information ni crÃ©ation de modalitÃ©s ambigÃ¼es.
En complÃ©ment, des flags binaires ont Ã©tÃ© ajoutÃ©s pour certaines variables clÃ©s, comme Â«â€¯Remuneration_missingâ€¯Â» ou Â«â€¯retirement_pay_missingâ€¯Â». Lâ€™objectif est de capter le signal potentiel portÃ© par la prÃ©sence mÃªme dâ€™une valeur manquante, parfois rÃ©vÃ©lateur dâ€™une logique de sous-dÃ©claration, dâ€™inadÃ©quation de la variable, ou dâ€™un profil atypique.
Ces flags ont ainsi pour but d'enrichir la modÃ©lisation en ajoutant une dimension mÃ©tadonnÃ©e.
Enfin, la variable Â«â€¯Statutâ€¯Â» a Ã©tÃ© encodÃ©e en binaire, permettant de combiner ou sÃ©parer les modÃ¨les selon les besoins, tout en assurant une compatibilitÃ© optimale dans les tests croisÃ©s ou les mÃ©ta-modÃ¨les.

Lâ€™ensemble des pipelines a Ã©tÃ© encapsulÃ©, sauvegardÃ©, et validÃ©, garantissant ainsi un enchaÃ®nement sans friction avec les Ã©tapes dâ€™entraÃ®nement supervisÃ©, et une possible rÃ©utilisation en production.

## Choix du modÃ¨le prÃ©dictif
Concernant le choix du modÃ¨le, diffÃ©rents modÃ¨les ont Ã©tÃ© testÃ©s, depuis la rÃ©gression logistique, utilisÃ©e comme modÃ¨le de base, jusqu'Ã  des algorithmes plus avancÃ©s comme LightGBM. D'autres ont Ã©tÃ© choisis pour leur complÃ©mentaritÃ© en termes dâ€™hypothÃ¨ses inductives. Par exemple, ExtraTrees privilÃ©gie une forte diversification des arbres via des splits totalement alÃ©atoires, favorisant la variance au dÃ©triment du biais, tandis que Balanced Random Forest applique un rÃ©Ã©chantillonnage Ã©quilibrÃ© afin de mieux gÃ©rer les classes dÃ©sÃ©quilibrÃ©es. Cette diversitÃ© permet dâ€™exploiter des dynamiques de dÃ©cision diffÃ©rentes et de limiter les corrÃ©lations entre modÃ¨les. Pour maximiser les performances de ces modÃ¨les, une optimisation des hyperparamÃ¨tres a Ã©tÃ© rÃ©alisÃ©e Ã  lâ€™aide dâ€™Optuna, y compris pour le mÃ©ta-modÃ¨le final. Les meilleurs paramÃ¨tres ont Ã©tÃ© conservÃ©s pour entraÃ®ner les modÃ¨les finaux sur lâ€™ensemble des donnÃ©es dâ€™apprentissage disponibles.

Les rÃ©sultats montrent une amÃ©lioration nette des performances avec les modÃ¨les boostÃ©s, notamment en prÃ©cision globale, F1-score et AUC-ROC â€” en particulier pour la classe majoritaire (classe 1). Par exemple, LightGBM atteint un AUC de 0.94 en global, et jusquâ€™Ã  0.99 dans lâ€™approche segmentÃ©e, avec un bon Ã©quilibre entre sensibilitÃ© et spÃ©cificitÃ©. Toutefois, ces performances quasi parfaites dans lâ€™approche diffÃ©renciÃ©e, notamment pour le statut Â«â€¯RetraitÃ©â€¯Â» (AUC â‰ˆ 1), soulÃ¨vent un possible problÃ©matique de surapprentissage.

Ces constats ont confortÃ© lâ€™idÃ©e de sâ€™orienter vers une approche par mÃ©ta-modÃ¨le, combinant les prÃ©dictions de modÃ¨les complÃ©mentaires â€” globaux et segmentÃ©s â€” afin dâ€™exploiter leur diversitÃ© informationnelle tout en rÃ©gulant les excÃ¨s dâ€™un modÃ¨le trop performant localement. Lâ€™objectif espÃ©rÃ© est de renforcer Ã  la fois la robustesse et la gÃ©nÃ©ralisabilitÃ© des prÃ©dictions sur les donnÃ©es de test.





# MÃ©ta-modÃ©lisation et analyse des performances


```{python}
#| label: importations-des-bibliotheques
#| echo: false
#| warning: false
#| message: false
#| output: false

# Librairies standard
import os
import sys

# Librairies scientifiques
import numpy as np
import pandas as pd

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from IPython.display import display, Markdown

# Scikit-learn
from sklearn.calibration import calibration_curve
from sklearn.inspection import permutation_importance
from sklearn.metrics import (
    brier_score_loss,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    f1_score,
    log_loss,
    PrecisionRecallDisplay,
    roc_auc_score,
    roc_curve
)

# Sauvegarde / chargement de modÃ¨les
import joblib
```


```{python}
#| label: import-module-pretraitement-global
#| echo: false
#| warning: false
#| message: false
#| output: false

# chemin complet vers le dossier
sys.path.append(r"C:/Users/vince/Documents/UniversitÃ© PSL/Paris_Dauphine-PSL/Apprentissage_supervisÃ©/Projet/donnees_projet_apprentissage_supervise")

# Import des fonctions
from pretraitement_global import (
    load_data,
    prepare_data,
    harmoniser_data_test,
    build_global_preprocessor
)
```


```{python}
#| label: chargement-donnees-modeles-calibres-et-predictions-oof
#| echo: false
#| warning: false
#| message: false
#| output: false

# Chemin vers le dossier contenant les fichiers : matrices et vecteurs sauvegardÃ©s, modÃ¨les calibrÃ©s, OOF sauvegardÃ©es
_, chemin_final = load_data()

if chemin_final.startswith("http"):
    raise ValueError("âŒ Les fichiers .pkl et .npy ne sont pas accessibles via GitHub. Veuillez exÃ©cuter en local.")
else:
    X_stack_actifs = pd.read_pickle(os.path.join(chemin_final, "X_stack_actifs.pkl"))
    X_stack_retraites = pd.read_pickle(os.path.join(chemin_final, "X_stack_retraites.pkl"))
    y_actifs = pd.read_pickle(os.path.join(chemin_final, "y_actifs.pkl"))
    y_retraites = pd.read_pickle(os.path.join(chemin_final, "y_retraites.pkl"))

    model_actifs = joblib.load(os.path.join(chemin_final, "meta_modele_actifs.pkl"))
    model_retraites = joblib.load(os.path.join(chemin_final, "meta_modele_retraites.pkl"))

    y_proba_oof_actifs = np.load(os.path.join(chemin_final, "y_proba_oof_actifs.npy"))
    y_proba_oof_retraites = np.load(os.path.join(chemin_final, "y_proba_oof_retraites.npy"))

    print("âœ… DonnÃ©es et modÃ¨les chargÃ©s avec succÃ¨s.")
```


```{python}
#| label: preparation-donnees-et-correlation-oof
#| echo: false
#| warning: false
#| message: false
#| output: false

from pretraitement_global import (
    load_data,
    prepare_data,
    harmoniser_data_test,
    build_global_preprocessor
)

# Chargement du jeu de donnÃ©es complet
df, chemin_base = load_data("jeu_donnees_final.csv")
df_test_raw, _ = load_data("jeu_donnees_final_test.csv")
df_test = harmoniser_data_test(df_test_raw)

# Chargement des prÃ©dictions OOF - Globales
oof_reglog        = np.load(os.path.join(chemin_base, "oof_pred_reglog.npy"))
oof_nbayes        = np.load(os.path.join(chemin_base, "oof_pred_nbayes.npy"))
oof_lgbm          = np.load(os.path.join(chemin_base, "oof_pred_lgbm.npy"))
oof_extratrees    = np.load(os.path.join(chemin_base, "oof_pred_extratrees.npy"))
oof_balanced_rf   = np.load(os.path.join(chemin_base, "oof_pred_brf.npy"))

# Ajout dans un DataFrame
df_global = pd.DataFrame({
    "OOF_RegLog": oof_reglog,
    "OOF_Nbayes_global": oof_nbayes,
    "OOF_LGBM_global": oof_lgbm,
    "OOF_ExtraTrees": oof_extratrees,
    "OOF_Balanced_RF": oof_balanced_rf
}, index=df.index)

# CorrÃ©lation globale
sns.heatmap(df_global.corr(), annot=True, cmap="coolwarm")
plt.title("CorrÃ©lation â€“ ModÃ¨les globaux (OOF)")
plt.tight_layout()
plt.show()

# Chargement OOF segmentÃ©es
oof_actifs = np.load(os.path.join(chemin_base, "oof_pred_actifs.npy"))
oof_retraites = np.load(os.path.join(chemin_base, "oof_pred_retraites.npy"))

# Ajout des modÃ¨les optimisÃ©s si prÃ©sents
try:
    oof_lgbm_optuna_actifs = np.load(os.path.join(chemin_base, "oof_pred_lgbm_optuna_actifs.npy"))
    oof_lgbm_optuna_retraites = np.load(os.path.join(chemin_base, "oof_pred_lgbm_optuna_retraites.npy"))
    optuna_present = True
except FileNotFoundError:
    optuna_present = False

# SÃ©paration des groupes
df_actifs = df[df["Statut"] == "Actif"].copy()
df_retraites = df[df["Statut"] == "RetraitÃ©"].copy()

# Ajout des OOF dans chaque sous-ensemble
df_actifs["OOF_segmentÃ©"] = oof_actifs
df_retraites["OOF_segmentÃ©"] = oof_retraites

for df_sub in [df_actifs, df_retraites]:
    for col in df_global.columns:
        df_sub[col] = df_global.loc[df_sub.index, col]

    # Ajout optuna si disponible
    if optuna_present:
        if df_sub is df_actifs:
            df_sub["OOF_LGBM_Optuna"] = oof_lgbm_optuna_actifs[df_sub.index]
        else:
            df_sub["OOF_LGBM_Optuna"] = oof_lgbm_optuna_retraites[df_sub.index]

# Fonction utilitaire pour la corrÃ©lation
def tracer_corr(df, colonnes, titre):
    corr = df[colonnes].corr()
    print(f"\n Matrice de corrÃ©lation â€“ {titre} :")
    print(corr)
    sns.heatmap(corr, annot=True, cmap="coolwarm")
    plt.title(f"CorrÃ©lation â€“ {titre}")
    plt.tight_layout()
    plt.show()

# Visualisation
colonnes_communes = ["OOF_RegLog", "OOF_Nbayes_global", "OOF_LGBM_global", "OOF_ExtraTrees", "OOF_Balanced_RF", "OOF_segmentÃ©"]
if optuna_present:
    colonnes_communes.append("OOF_LGBM_Optuna")

tracer_corr(df_actifs, colonnes_communes, "Actifs")
tracer_corr(df_retraites, colonnes_communes, "RetraitÃ©s")
```


## Matrices des correlations des prÃ©dictions OOF


```{python}
#| label: matrices-correlation-oof
#| echo: false
#| warning: false
#| message: false

# DÃ©finition des colonnes
colonnes_communes = ["OOF_RegLog", "OOF_Nbayes_global", "OOF_LGBM_global", "OOF_ExtraTrees", "OOF_Balanced_RF", "OOF_segmentÃ©"]
if optuna_present:
    colonnes_communes.append("OOF_LGBM_Optuna")

# CrÃ©ation d'une figure Ã  3 sous-graphes
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# CorrÃ©lation globale
corr_globale = df_global.corr()
sns.heatmap(corr_globale, annot=True, cmap="coolwarm", ax=axes[0])
axes[0].set_title("CorrÃ©lation â€“ ModÃ¨les globaux", color='navy')

# CorrÃ©lation actifs
corr_actifs = df_actifs[colonnes_communes].corr()
sns.heatmap(corr_actifs, annot=True, cmap="coolwarm", ax=axes[1])
axes[1].set_title("CorrÃ©lation â€“ Actifs", color='navy')

# CorrÃ©lation retraitÃ©s
corr_retraites = df_retraites[colonnes_communes].corr()
sns.heatmap(corr_retraites, annot=True, cmap="coolwarm", ax=axes[2])
axes[2].set_title("CorrÃ©lation â€“ RetraitÃ©s", color='navy')

plt.tight_layout()
plt.show()
```


Les matrices des corrÃ©lations entre prÃ©dictions out-of-fold (OOF) soulignent des diffÃ©rences nettes entre les modÃ¨les selon les sous-populations â€” actifs vs retraitÃ©s. Les comportements des modÃ¨les sont parfois cohÃ©rents (fortement corrÃ©lÃ©s), parfois trÃ¨s divergents (faiblement corrÃ©lÃ©s), ce qui suggÃ¨re des dynamiques dâ€™apprentissage diffÃ©rentes selon les profils. Face Ã  ces constats, le modÃ¨le a Ã©tÃ© construit de maniÃ¨re Ã  :

- construire deux mÃ©ta-modÃ¨les sÃ©parÃ©s, chacun spÃ©cialisÃ© selon le statut socio-Ã©conomique ;

- intÃ©grer explicitement la variable Â«â€¯Statutâ€¯Â» dans chaque mÃ©ta-modÃ¨le pour en prÃ©server la trace structurelle.

Cette architecture permet de spÃ©cialiser lâ€™agrÃ©gation des prÃ©dictions tout en conservant une cohÃ©rence globale du systÃ¨me de dÃ©cision.

Pour chaque sous-population, les prÃ©dictions de plusieurs modÃ¨les ont Ã©tÃ© combinÃ©es de maniÃ¨re diffÃ©renciÃ©e, sur la base de deux critÃ¨res principaux :
- la performance individuelle de chaque modÃ¨le sur le segment concernÃ©,
- sa complÃ©mentaritÃ© structurelle, mesurÃ©e par sa corrÃ©lation avec les autres prÃ©dicteurs (OOF).

Les modÃ¨les conservÃ©s dans les matrices de stacking sont donc ceux qui apportaient Ã  la fois un signal informatif et une diversitÃ© utile.

Chez les actifs, les modÃ¨les retenus comprennent la rÃ©gression logistique, le modÃ¨le LightGBM, ExtraTrees, la forÃªt alÃ©atoire pondÃ©rÃ©e (Balanced Random Forest) ainsi que le modÃ¨le segmentÃ©. La rÃ©gression logistique a Ã©tÃ© conservÃ©e pour son bon calibrage et sa robustesse, fournissant une base linÃ©aire stable dans la combinaison. Le modÃ¨le LightGBM sâ€™est imposÃ© comme le plus performant sur ce segment, avec les meilleurs scores en AUC et F1-score, constituant le cÅ“ur du signal prÃ©dictif. Les modÃ¨les ExtraTrees et Balanced Random Forest apportent une diversification utile : le premier par des splits alÃ©atoires favorisant la variance, le second par sa capacitÃ© Ã  gÃ©rer efficacement le dÃ©sÃ©quilibre entre classes, ce qui est un des points d'attention du fait du lÃ©ger dÃ©sÃ©quilibre prÃ©sente dans la distribution de la variable cible. Le modÃ¨le segmentÃ©, entraÃ®nÃ© exclusivement sur la population active, a Ã©tÃ© intÃ©grÃ© pour enrichir la prÃ©diction par un signal spÃ©cialisÃ©. En revanche, le modÃ¨le Naive Bayes a Ã©tÃ© Ã©cartÃ© dans ce segment. Bien quâ€™il gÃ©nÃ¨re parfois un signal orthogonal utile en stacking, ses performances individuelles se sont rÃ©vÃ©lÃ©es nettement infÃ©rieures (AUC = 0.76), avec une expressivitÃ© limitÃ©e et une corrÃ©lation faible avec la cible. De plus, son rappel Ã©levÃ© sur la classe 0 se paie par une chute importante de prÃ©cision, ce qui introduit un grand nombre de faux positifs. Dans un segment oÃ¹ les autres modÃ¨les offrent dÃ©jÃ  un excellent Ã©quilibre entre sensibilitÃ© et prÃ©cision, ce dÃ©sÃ©quilibre aurait risquÃ© de dÃ©tÃ©riorer la qualitÃ© globale des prÃ©dictions. Il a donc Ã©tÃ© jugÃ© quâ€™il nâ€™apportait ni gain informationnel suffisant, ni complÃ©mentaritÃ© stratÃ©gique justifiant son inclusion dans la combinaison finale.

Chez les retraitÃ©s, la sÃ©lection finale a inclus ces mÃªmes modÃ¨les de base, mais avec le maintien du Naive Bayes. Comme pour les actifs, la rÃ©gression logistique apporte un socle interprÃ©table, avec des rÃ©sultats corrects sur les deux classes. LightGBM domine largement en termes de performance sur cette population. ExtraTrees et Balanced Random Forest ont Ã©tÃ© retenus pour leur capacitÃ© Ã  diversifier le signal, en sâ€™appuyant sur des partitions distinctes et une logique de pondÃ©ration utile face Ã  lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© de la population retraitÃ©e. Le modÃ¨le segmentÃ© retraitÃ©s, malgrÃ© ses performances quasi parfaites, a Ã©tÃ© intÃ©grÃ© avec prÃ©caution dans le stacking, afin de profiter de sa finesse sans risquer une surspÃ©cialisation. Enfin, le modÃ¨le Naive Bayes a Ã©tÃ© conservÃ© spÃ©cifiquement dans ce segment, non pour ses performances brutes, mais pour sa trÃ¨s faible corrÃ©lation avec les autres modÃ¨les - quasi inexistante sur ce segment. Ce caractÃ¨re orthogonal en fait un candidat intÃ©ressant dans une logique dâ€™agrÃ©gation. De plus, contrairement au cas des actifs, son rappel Ã©levÃ© sur la classe 0 sâ€™accompagne ici dâ€™une dÃ©gradation de la prÃ©cision moins risquÃ©e du fait, notamment, de la performance trÃ¨s solide du modÃ¨le diffÃ©renciÃ© sur ce segment, ce qui limite le risque de perturbation du modÃ¨le global. Sa prÃ©sence pourrait ainsi permettre au mÃ©ta-modÃ¨le de disposer dâ€™un signal complÃ©mentaire, potentiellement utile pour discriminer certains profils marginaux ou atypiques. Sa contribution reste en outre rÃ©gulÃ©e par le stacking, afin dâ€™Ã©viter que ses faiblesses sur la classe positive nâ€™altÃ¨rent la performance globale.

Cette stratÃ©gie de sÃ©lection diffÃ©renciÃ©e a pour but de maximiser la complÃ©mentaritÃ© fonctionnelle entre les modÃ¨les, tout en respectant les dynamiques propres Ã  chaque groupe. Elle devrait renforcer la capacitÃ© du mÃ©ta-modÃ¨le Ã  gÃ©nÃ©raliser de maniÃ¨re robuste, sans sacrifier la sensibilitÃ© spÃ©cifique Ã  chaque sous-population.

## Recherche des F1-score optimaux

```{python}
#| label: rechercher-seuils-F1score-optimaux
#| echo: false
#| warning: false
#| message: false

# Fonction pour rechercher le seuil F1 optimal
def seuil_f1_optimal(y_observe, y_proba):
    seuils = np.linspace(0.05, 0.95, 400)
    scores_f1 = [f1_score(y_observe, y_proba > s) for s in seuils]
    indice_optimal = np.argmax(scores_f1)
    meilleur_seuil = seuils[indice_optimal]
    meilleur_f1 = scores_f1[indice_optimal]
    return meilleur_seuil, meilleur_f1, seuils, scores_f1

# Recherche du seuil optimal pour les deux groupes
seuil_opt_actifs, f1_max_actifs, tous_seuils_actifs, tous_scores_actifs = seuil_f1_optimal(y_actifs, y_proba_oof_actifs)
seuil_opt_retraites, f1_max_retraites, tous_seuils_retraites, tous_scores_retraites = seuil_f1_optimal(y_retraites, y_proba_oof_retraites)

print(f"ğŸ” Seuil optimal F1 (actifs) : {seuil_opt_actifs:.3f}")
print(f"âœ… F1-score maximal obtenu : {f1_max_actifs:.4f}")

print(f"ğŸ” Seuil optimal F1 (retraitÃ©s) : {seuil_opt_retraites:.3f}")
print(f"âœ… F1-score maximal obtenu : {f1_max_retraites:.4f}")

# Affichage cÃ´te Ã  cÃ´te
fig, axes = plt.subplots(1, 2, figsize=(16, 5))

# Courbe Actifs
axes[0].plot(tous_seuils_actifs, tous_scores_actifs, label="Score F1")
axes[0].axvline(seuil_opt_actifs, color="r", linestyle="--", label=f"Seuil optimal = {seuil_opt_actifs:.2f}")
axes[0].set_title("Courbe F1 selon le seuil â€“ Actifs", color='navy')
axes[0].set_xlabel("Seuil de dÃ©cision")
axes[0].set_ylabel("Score F1")
axes[0].legend()
axes[0].grid(True)

# Courbe RetraitÃ©s
axes[1].plot(tous_seuils_retraites, tous_scores_retraites, label="Score F1")
axes[1].axvline(seuil_opt_retraites, color="r", linestyle="--", label=f"Seuil optimal = {seuil_opt_retraites:.2f}")
axes[1].set_title("Courbe F1 selon le seuil â€“ RetraitÃ©s", color='navy')
axes[1].set_xlabel("Seuil de dÃ©cision")
axes[1].set_ylabel("Score F1")
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()
```


Afin dâ€™amÃ©liorer la performance finale des mÃ©ta-modÃ¨les, une recherche du seuil de dÃ©cision optimal a Ã©tÃ© menÃ©e sÃ©parÃ©ment pour les actifs et les retraitÃ©s, Ã  partir des prÃ©dictions out-of-fold. Cette Ã©tape vise Ã  maximiser le F1-score, pour sa capacitÃ© Ã  Ã©quilibrer la prÃ©cision et le rappel. Ce choix est privilÃ©giÃ© ici, car dans un contexte de classes dÃ©sÃ©quilibrÃ©es, lâ€™accuracy peut se rÃ©vÃ©ler trompeuse car trop optimiste, et lâ€™AUC ne pas suffisamment traduire directement la performance rÃ©elle dâ€™un seuil de dÃ©cision particulier. Le F1-score, en tant que moyenne harmonique, devrait permettre de privilÃ©gier une dÃ©tection efficace des cas positifs tout en pÃ©nalisant plus fortement les dÃ©sÃ©quilibres entre prÃ©cision et rappel.

Chez les actifs, le seuil optimal maximisant le F1-score sâ€™Ã©tablit Ã  0.42, avec un score maximal de 0.9019. La courbe prÃ©sente une asymÃ©trie modÃ©rÃ©e, montrant que le seuil canonique de 0.5 (i.e. la valeur par dÃ©faut utilisÃ©e pour transformer une probabilitÃ© en dÃ©cision binaire) nâ€™est pas adaptÃ© ici. Le choix dâ€™un seuil plus bas reflÃ¨te une volontÃ© dâ€™amÃ©liorer la sensibilitÃ© du modÃ¨le dans un segment oÃ¹ la classe minoritaire est plus difficile Ã  dÃ©tecter.

Chez les retraitÃ©s, la courbe du F1-score est plus rÃ©guliÃ¨re, avec un maximum atteint pour un seuil lÃ©gÃ¨rement supÃ©rieur, Ã  0.54, et un F1-score maximal de 0.9315. Cette proximitÃ© avec le seuil de 0.5 suggÃ¨re une meilleure sÃ©paration des classes dans les distributions de probabilitÃ©s : le modÃ¨le semble distinguer plus nettement les cas positifs des cas nÃ©gatifs, rendant lâ€™ajustement du seuil moins critique.

Cette Ã©tape dâ€™optimisation devrait ainsi permettre dâ€™ajuster plus finement la stratÃ©gie de dÃ©cision du mÃ©ta-modÃ¨le global, en tenant compte des spÃ©cificitÃ©s de chaque sous-population. En adaptant le seuil selon le profil de chaque individus (actif ou retraitÃ©), elle vise Ã  amÃ©liorer lâ€™Ã©quilibre des prÃ©dictions, leur cohÃ©rence interne, et la pertinence opÃ©rationnelle des dÃ©cisions automatisÃ©es qui en dÃ©couleront en production.

## MÃ©triques finales â€“ stacking calibrÃ© sur F1-score optimaux

```{python}
#| label: Ã‰valuation-meta-modÃ¨le-segmentÃ©-avec-seuils-optimaux_F1score
#| echo: false
#| warning: false
#| message: false

# ============================================
# Seuils optimaux F1-score
# ============================================
seuil_opt_actifs = 0.424
seuil_opt_retraites = 0.537

# ============================================
# PrÃ©dictions (probabilitÃ©s et binaires)
# ============================================
proba_actifs = model_actifs.predict_proba(X_stack_actifs)[:, 1]
proba_retraites = model_retraites.predict_proba(X_stack_retraites)[:, 1]

pred_actifs = (proba_actifs >= seuil_opt_actifs).astype(int)
pred_retraites = (proba_retraites >= seuil_opt_retraites).astype(int)

# Global (concatÃ©nation)
y_reel_global = pd.concat([y_actifs, y_retraites])
y_proba_global = pd.concat([
    pd.Series(proba_actifs, index=y_actifs.index),
    pd.Series(proba_retraites, index=y_retraites.index)
])
y_pred_global = pd.concat([
    pd.Series(pred_actifs, index=y_actifs.index),
    pd.Series(pred_retraites, index=y_retraites.index)
])

# ============================================
# Fonction dâ€™extraction des indicateurs
# ============================================
def extraire_resume_complet(y_true, y_pred, y_proba, label):
    rapport = classification_report(y_true, y_pred, output_dict=True)
    cm = confusion_matrix(y_true, y_pred)
    
    # SpÃ©cificitÃ© = TN / (TN + FP)
    tn, fp, fn, tp = cm.ravel()
    specificite = tn / (tn + fp) if (tn + fp) > 0 else np.nan
    
    return {
        "Statut": label,
        "Accuracy": round(rapport["accuracy"], 3),
        "PrÃ©cision (1)": round(rapport["1"]["precision"], 3),
        "SensibilitÃ© / Rappel (1)": round(rapport["1"]["recall"], 3),
        "SpÃ©cificitÃ© (0)": round(specificite, 3),
        "F1-score (1)": round(rapport["1"]["f1-score"], 3),
        "F1 pondÃ©rÃ©": round(rapport["weighted avg"]["f1-score"], 3),
        "AUC": round(roc_auc_score(y_true, y_proba), 3)
    }

# ============================================
# Matrices de confusion
# ============================================
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

matrices = [
    confusion_matrix(y_actifs, pred_actifs),
    confusion_matrix(y_retraites, pred_retraites),
    confusion_matrix(y_reel_global, y_pred_global)
]

titres = [
    "Actifs (optimisÃ© seuil F1)",
    "RetraitÃ©s (optimisÃ© seuil F1)",
    "Global (optimisÃ© seuils F1 diffÃ©renciÃ©s)"
]

for ax, cm, titre in zip(axes, matrices, titres):
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=ax, cmap="Blues", colorbar=False)
    ax.set_title(f"Matrice de confusion â€” {titre}", color='navy')
    ax.grid(False)

plt.tight_layout()
plt.show()

# ============================================
# Courbes ROC avec AUC
# ============================================
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, y_true, y_score, titre in zip(
    axes,
    [y_actifs, y_retraites, y_reel_global],
    [proba_actifs, proba_retraites, y_proba_global],
    ["Actifs", "RetraitÃ©s", "Global"]
):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    auc = roc_auc_score(y_true, y_score)
    ax.plot(fpr, tpr, label=f"AUC = {auc:.3f}")
    ax.plot([0, 1], [0, 1], "--", color="gray")
    ax.set_title(f"Courbe ROC â€” {titre}", color='navy')
    ax.set_xlabel("1 - SpÃ©cificitÃ©")
    ax.set_ylabel("SensibilitÃ©")
    ax.legend()
    ax.grid(True)

plt.tight_layout()
plt.show()

# ============================================
# RÃ©sumÃ© comparatif des performances
# ============================================
ligne_actifs = extraire_resume_complet(y_actifs, pred_actifs, proba_actifs, "Actifs")
ligne_retraites = extraire_resume_complet(y_retraites, pred_retraites, proba_retraites, "RetraitÃ©s")
ligne_global = extraire_resume_complet(y_reel_global, y_pred_global, y_proba_global, "Global")

df_eval_comparative = pd.DataFrame([ligne_actifs, ligne_retraites, ligne_global])

# Mise en forme
df_affichage = df_eval_comparative.set_index("Statut").round(3).rename(columns={
    "Accuracy": "Exactitude",
    "PrÃ©cision (1)": "PrÃ©cision",
    "SensibilitÃ© / Rappel (1)": "Rappel",
    "SpÃ©cificitÃ© (0)": "SpÃ©cificitÃ©",
    "F1-score (1)": "F1-score",
    "F1 pondÃ©rÃ©": "F1 pondÃ©rÃ©",
    "AUC": "AUC"
})

display(
    df_affichage.style
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center"), ("border-bottom", "1px solid black")]},
        {"selector": "td", "props": [("text-align", "center")]}
    ])
    .set_properties(**{"font-size": "10pt"})
)
```


Lâ€™Ã©valuation finale des mÃ©ta-modÃ¨les segmentÃ©s, aprÃ¨s calibration des seuils optimaux du F1-score, met en Ã©vidence un Ã©quilibre solide, aussi bien au niveau individuel (actifs et retraitÃ©s) que global.

Chez les actifs, le modÃ¨le atteint un F1-score de 0.91, avec une prÃ©cision de 0.88 et un rappel de 0.93. Cette configuration traduit une trÃ¨s bonne capacitÃ© Ã  dÃ©tecter les cas positifs tout en limitant les erreurs. Le score de spÃ©cificitÃ© (0.78) reste modÃ©rÃ© mais acceptable, en cohÃ©rence avec le seuil optimisÃ© Ã  0.424 qui favorise la sensibilitÃ©. Le modÃ¨le assume ainsi un compromis stratÃ©gique : tolÃ©rer davantage de faux positifs sur la classe 0 pour maximiser la dÃ©tection des cas pertinents, dans un segment oÃ¹ la classe positive est majoritaire.

Chez les retraitÃ©s, les rÃ©sultats sont encore plus convaincants. Le modÃ¨le atteint un F1-score de 0.96, avec une prÃ©cision de 0.94 et un rappel de 0.97, traduisant une excellente rÃ©gularitÃ© dans la dÃ©tection des cas positifs. La spÃ©cificitÃ© de 0.72, lÃ©gÃ¨rement infÃ©rieure Ã  celle des actifs, est compensÃ©e par la trÃ¨s forte sensibilitÃ© du modÃ¨le. Cette configuration tÃ©moigne dâ€™une sÃ©paration claire entre les classes dans lâ€™espace des probabilitÃ©s prÃ©dictives, facilitant lâ€™arbitrage par seuil et renforÃ§ant lâ€™efficacitÃ© de la combinaison des modÃ¨les sous-jacents.

La fusion des prÃ©dictions segmentÃ©es, avec seuils optimisÃ©s pour chaque groupe, conduit Ã  un F1-score global de 0.92, une accuracy de 0.89, et une AUC de 0.96. La sensibilitÃ© globale atteint 0.95, confirmant la capacitÃ© du systÃ¨me Ã  capter efficacement les cas positifs. La spÃ©cificitÃ©, quant Ã  elle, sâ€™Ã©tablit Ã  0.77, cohÃ©rente avec les arbitrages opÃ©rÃ©s localement dans chaque segment.

Les matrices de confusion et les courbes ROC illustrent visuellement cette dynamique : les erreurs sont bien rÃ©parties, sans dÃ©sÃ©quilibre majeur, et la zone sous la courbe reste Ã©levÃ©e dans les trois cas, gage dâ€™une forte capacitÃ© de discrimination.

Lâ€™ensemble de ces rÃ©sultats met en Ã©vidence les gains substantiels permis par la segmentation statutaire et la calibration fine des seuils de dÃ©cision :

- La rÃ©gression logistique seule, bien que robuste et bien calibrÃ©e, plafonnait Ã  un F1-score de 0.87 pour la classe positive, avec une spÃ©cificitÃ© limitÃ©e Ã  0.58 sur la classe minoritaire. Ce niveau relativement bas de spÃ©cificitÃ© traduisait une tendance marquÃ©e Ã  la surdÃ©tection des cas positifs â€” autrement dit, une production excessive de faux positifs â€” malgrÃ© un trÃ¨s bon rappel (0.92) sur la classe 1. Ce dÃ©sÃ©quilibre, symptomatique de lâ€™usage dâ€™un seuil par dÃ©faut mal adaptÃ©, justifie pleinement la nÃ©cessitÃ© dâ€™une optimisation diffÃ©renciÃ©e des seuils et dâ€™une modÃ©lisation segmentÃ©e selon les profils.

- Le modÃ¨le LightGBM global offrait dÃ©jÃ  une performance plus solide, atteignant un F1-score de 0.91 sur la classe positive, une AUC de 0.94, et un meilleur compromis global. Toutefois, son F1-score pondÃ©rÃ© (0.87) et sa spÃ©cificitÃ© encore modÃ©rÃ©e (0.74) montraient quâ€™une marge de progression subsistait, en particulier sur les cas nÃ©gatifs.

Le passage Ã  une modÃ©lisation diffÃ©renciÃ©e par statut â€” mÃªme sans stacking â€” a permis une nette avancÃ©e : le F1-score atteignait 0.95 chez les actifs et 0.99 chez les retraitÃ©s, avec des AUC trÃ¨s robustes de 0.9897 et 0.9995 respectivement. Ces rÃ©sultats traduisaient une excellente capacitÃ© de discrimination, notamment chez les retraitÃ©s, oÃ¹ les classes Ã©taient quasi parfaitement sÃ©parÃ©es.

Câ€™est toutefois lâ€™Ã©tape finale, combinant stacking spÃ©cialisÃ© par sous-population et optimisation des seuils F1, qui a permis de consolider ces performances et dâ€™en renforcer la robustesse. Le F1-score global atteint dÃ©sormais 0.92, avec une AUC de 0.96. Surtout, lâ€™Ã©quilibre entre sensibilitÃ© (0.95) et spÃ©cificitÃ© (0.77) sâ€™est nettement amÃ©liorÃ©, assurant une dÃ©tection efficace des cas positifs sans compromettre la prÃ©cision sur les cas nÃ©gatifs.

Ces progrÃ¨s reflÃ¨tent lâ€™apport complÃ©mentaire des modÃ¨les agrÃ©gÃ©s, la pertinence de la segmentation et la valeur ajoutÃ©e dâ€™une calibration diffÃ©renciÃ©e. Lâ€™approche par segmentation, couplÃ©e Ã  un ajustement fin des seuils de dÃ©cision, a permis dâ€™atteindre une performance Ã©levÃ©e tout en conservant une cohÃ©rence mÃ©tier et statistique dans le traitement diffÃ©renciÃ© des profils actifs et retraitÃ©s. En somme, les rÃ©sultats suggÃ¨rent que le systÃ¨me modulaire proposÃ© est robuste, Ã  la fois performant, explicable et adaptÃ© aux spÃ©cificitÃ©s des populations cibles.


```{python}
#| label: Evaluation-avancÃ©e-des-performances-modÃ¨le-stackÃ©-diffÃ©renciÃ©-calibrÃ©-par-F1-score-optimaux
#| echo: false
#| warning: false
#| message: false

# Fonction gÃ©nÃ©rique
def evaluer_modele_complet(nom_modele, modele, X, y, seuil=None, couleur='navy', afficher_importance=True):
    print(f"\nğŸ” Ã‰valuation avancÃ©e â€” modÃ¨le stackÃ© {nom_modele}")

    # PrÃ©dictions de probabilitÃ©
    y_proba = modele.predict_proba(X)[:, 1]
    
    # Brier Score & Log Loss (toujours affichÃ©s)
    print(f"ğŸ” Brier Score : {brier_score_loss(y, y_proba):.4f}")
    print(f"ğŸ” Log Loss     : {log_loss(y, y_proba):.4f}")

    # PrÃ©dictions binaires si seuil donnÃ©
    y_pred = None
    if seuil is not None:
        y_pred = (y_proba >= seuil).astype(int)
        print(f"âœ… Seuil utilisÃ© : {seuil:.3f}")

        # Classification report
        print("\n Rapport de classification :")
        print(classification_report(y, y_pred))

    # Courbes : Calibration et PrÃ©cision-Rappel
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Courbe de calibration
    prob_true, prob_pred = calibration_curve(y, y_proba, n_bins=10)
    axes[0].plot(prob_pred, prob_true, marker="o")
    axes[0].plot([0, 1], [0, 1], "--", color="gray")
    axes[0].set_xlabel("PrÃ©diction moyenne")
    axes[0].set_ylabel("ProbabilitÃ© empirique")
    axes[0].set_title(f"Calibration â€” {nom_modele}", color=couleur)
    axes[0].grid(True)

    # Courbe prÃ©cision-rappel
    PrecisionRecallDisplay.from_predictions(y, y_proba, ax=axes[1])
    axes[1].set_title(f"Courbe PrÃ©cision-Rappel â€” {nom_modele}", color=couleur)

    plt.tight_layout()
    plt.show()

    # Importance par permutation
    if afficher_importance:
        try:
            result = permutation_importance(modele, X, y, n_repeats=10, random_state=42)
            importances = pd.Series(result.importances_mean, index=X.columns).sort_values()
            importances.plot(kind="barh", title=f"Importance des variables â€” permutation ({nom_modele})", color="purple")
            plt.title(f"Importance des variables â€” permutation ({nom_modele})", color=couleur)
            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"âš ï¸ Importance par permutation non affichÃ©e : {e}")
            
            
# GÃ©nÃ©ration des rÃ©sultats
# Actifs
evaluer_modele_complet("Actifs", model_actifs, X_stack_actifs, y_actifs, seuil=0.424)

# RetraitÃ©s
evaluer_modele_complet("RetraitÃ©s", model_retraites, X_stack_retraites, y_retraites, seuil=0.537)

# ModÃ¨le combinÃ© (sans permutation importance ni seuil)
class ModeleCombinÃ©:
    def predict_proba(self, X):
        y_proba = np.concatenate([
            model_actifs.predict_proba(X_stack_actifs)[:, 1],
            model_retraites.predict_proba(X_stack_retraites)[:, 1]
        ])
        return np.column_stack([1 - y_proba, y_proba])

# Global combinÃ©
y_globale = np.concatenate([y_actifs, y_retraites])
X_globale = pd.concat([X_stack_actifs, X_stack_retraites], axis=0)

evaluer_modele_complet(
    "Global diffÃ©renciÃ© (calibrÃ©)",
    ModeleCombinÃ©(),
    X_globale,
    y_globale,
    afficher_importance=False
)
```


En complÃ©ment, les courbes de calibration montrent une bonne concordance entre les probabilitÃ©s prÃ©dites et les observations rÃ©elles, notamment chez les actifs. Cela confirme que le mÃ©ta-modÃ¨le ne se contente pas dâ€™Ãªtre performant en termes de classification (F1-score, AUC), mais quâ€™il est Ã©galement bien calibrÃ©, ce qui est essentiel dans toute prise de dÃ©cision fondÃ©e sur un seuil probabiliste. Les scores de Brier faibles (0.084 pour les actifs, 0.057 pour les retraitÃ©s) ainsi que les valeurs de log loss renforcent cette impression de fiabilitÃ©.

Les courbes prÃ©cision-rappel, quant Ã  elles, permettent de juger plus finement la performance dans un contexte de classes dÃ©sÃ©quilibrÃ©es. Les scores *Average Precision* atteignent 0.97 chez les actifs et 0.99 chez les retraitÃ©s, traduisant une capacitÃ© Ã  maintenir une prÃ©cision Ã©levÃ©e mÃªme pour des niveaux de rappel trÃ¨s Ã©levÃ©s. En dâ€™autres termes, le systÃ¨me parvient Ã  dÃ©tecter efficacement un grand nombre de cas positifs sans pour autant dÃ©grader la fiabilitÃ© des alertes Ã©mises. Ce niveau de performance est particuliÃ¨rement prÃ©cieux dans un contexte opÃ©rationnel sensible, oÃ¹ les faux positifs reprÃ©sentent un coÃ»t, mais oÃ¹ les faux nÃ©gatifs peuvent avoir des consÃ©quences critiques.

Lâ€™analyse par permutation apporte un Ã©clairage sur la contribution diffÃ©renciÃ©e des prÃ©dicteurs dans le mÃ©ta-modÃ¨le selon les sous-populations.

- Chez les actifs, le prÃ©dicteur segmentÃ© â€” câ€™est-Ã -dire lâ€™agrÃ©gation explicite des modÃ¨les optimisÃ©s localement â€” sâ€™impose comme le contributeur principal au score final, devant mÃªme les meilleurs modÃ¨les individuels comme LightGBM. Cela montre que la logique de spÃ©cialisation locale est ici ce qui apporte le plus de valeur, probablement en raison de lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© structurelle de cette population.

- Chez les retraitÃ©s, Ã  lâ€™inverse, câ€™est le modÃ¨le LightGBM qui ressort comme le plus influent, devant le prÃ©dicteur segmentÃ©. Cela suggÃ¨re quâ€™une structure plus homogÃ¨ne dans cette population permet Ã  un seul modÃ¨le puissant de capter lâ€™essentiel du signal, sans nÃ©cessiter une spÃ©cialisation aussi poussÃ©e.

Ce contraste rÃ©vÃ¨le une forme dâ€™adaptation hiÃ©rarchique :

- Dans les segments plus complexes ou instables (comme les actifs), câ€™est lâ€™architecture segmentÃ©e qui prend lâ€™ascendant ;

- Tandis que dans les segments plus rÃ©guliers (comme les retraitÃ©s), la domination dâ€™un modÃ¨le robuste unique suffit Ã  maintenir une trÃ¨s haute performance.

Ces constats renforcent la lÃ©gitimitÃ© du recours Ã  une architecture de mÃ©ta-modÃ©lisation segmentÃ©e : non seulement cette approche exploite au mieux la diversitÃ© des prÃ©dicteurs, mais elle permet surtout dâ€™adapter finement la logique de dÃ©cision aux dynamiques propres Ã  chaque population, en combinant spÃ©cialisation et agrÃ©gation de maniÃ¨re optimale.




# Pour aller plus loin

## Analyse SHAP sur le modÃ¨le stackÃ© â€” Actifs


```{python}
#| label: Analyse-SHAP-modÃ¨le-stackÃ©-actifs
#| echo: false
#| warning: false
#| message: false

print("\n Analyse SHAP sur le modÃ¨le stackÃ© â€” Actifs")

# MmodÃ¨le stackÃ© final
modele_shap = model_actifs

explainer = shap.Explainer(model_actifs, X_stack_actifs, algorithm="tree")
shap_values = explainer(X_stack_actifs, check_additivity=False)

# RÃ©sumÃ© global
shap.plots.bar(shap_values, max_display=10)
shap.plots.beeswarm(shap_values, max_display=10)

# Zoom sur un faux positif
prediction_binaire_actifs  = model_actifs.predict(X_stack_actifs)
faux_positifs = (y_actifs == 0) & (prediction_binaire_actifs  == 1)

if faux_positifs.any():
    faux_positifs_idx = np.where(faux_positifs)[0][0]
    vrai_idx = y_actifs.index[faux_positifs_idx]
    print(f"\nğŸ” Zoom sur un faux positif â€” index {vrai_idx}")
    shap.plots.waterfall(shap_values[faux_positifs_idx])
else:
    print("âœ… Aucun faux positif dÃ©tectÃ©.")
```


Lâ€™analyse SHAP du mÃ©ta-modÃ¨le apporte un Ã©clairage complÃ©mentaire sur les dynamiques internes du systÃ¨me, en confirmant lâ€™importance diffÃ©renciÃ©e des prÃ©dicteurs selon les segments. Chez les actifs, le prÃ©dicteur segmentÃ© domine trÃ¨s nettement en valeur SHAP moyenne, devant les modÃ¨les de base comme LGBM ou ExtraTrees. Ce constat renforce lâ€™idÃ©e que la spÃ©cialisation des modÃ¨les, adaptÃ©e au statut des individus, constitue la principale source de performance pour cette sous-population hÃ©tÃ©rogÃ¨ne. Le beeswarm plot montre une forte dispersion des contributions locales, en particulier pour les modÃ¨les SegmentÃ© et LGBM. Cette variabilitÃ© traduit un comportement adaptatif : le poids relatif de chaque modÃ¨le varie sensiblement selon les individus, signe que le mÃ©ta-modÃ¨le ajuste sa dÃ©cision en fonction des spÃ©cificitÃ©s de chaque profil. Autrement dit, les actifs ne semblent pas suivre un schÃ©ma unique, et le systÃ¨me apprend Ã  pondÃ©rer les prÃ©dicteurs de maniÃ¨re contextuelle. Cette influence se manifeste aussi Ã  lâ€™Ã©chelle individuelle, oÃ¹ lâ€™examen dâ€™un faux positif illustre cette dynamique : le prÃ©dicteur segmentÃ© oriente fortement vers la classe 0, mais RegLog pousse vers la classe 1, rÃ©vÃ©lant des dÃ©saccords internes que le mÃ©ta-modÃ¨le tranche. On observe ainsi une forme de pondÃ©ration locale, qui ajuste la dÃ©cision en fonction des signaux dominants.

Ã€ lâ€™inverse, chez les retraitÃ©s (les rÃ©sultats sont prÃ©sentÃ©s en partie III.f du notebook associÃ©), câ€™est LGBM qui sâ€™impose comme contributeur principal, y compris en importance SHAP globale. Le modÃ¨le segmentÃ© y conserve nÃ©anmoins une place significative, traduisant une valeur ajoutÃ©e ponctuelle dans certains cas limites. Cette hiÃ©rarchie plus aplatie entre prÃ©dicteurs reflÃ¨te une dynamique plus linÃ©aire et moins segmentÃ©e, en cohÃ©rence avec une population plus stable et homogÃ¨ne. Le beeswarm plot illustre cette cohÃ©rence par une dispersion rÃ©duite des contributions locales, principalement tirÃ©e par LGBM, puis ExtraTrees et le modÃ¨le segmentÃ©. Enfin, lâ€™analyse dâ€™un faux positif rÃ©vÃ¨le un cas typique dâ€™arbitrage entre prÃ©dicteurs divergents : ici, le modÃ¨le LGBM penche vers la classe 0, tandis que RegLog et SegmentÃ© tirent vers la classe 1. Cette configuration illustre la capacitÃ© du mÃ©ta-modÃ¨le Ã  combiner des signaux parfois contradictoires, au risque dâ€™erreurs marginales mais structurellement explicables. Ce cas particulier met en Ã©vidence un phÃ©nomÃ¨ne dâ€™arbitrage structurel au sein du modÃ¨le stackÃ© : le prÃ©dicteur LGBM, qui domine globalement chez les retraitÃ©s, peut parfois exercer une influence prÃ©pondÃ©rante mÃªme lorsque d'autres modÃ¨les, tels que RegLog ou SegmentÃ©, captent un signal inverse. Dans cet exemple de faux positif, la contribution fortement nÃ©gative de LGBM tire la prÃ©diction vers la classe 0, neutralisant lâ€™apport positif dâ€™autres prÃ©dicteurs. Cela souligne Ã  la fois la puissance du consensus majoritaire dans un stacking, mais aussi ses limites ponctuelles lorsquâ€™un modÃ¨le principal se trompe avec confiance.

Lâ€™introduction du prÃ©dicteur segmentÃ© ne constitue donc pas uniquement un levier de performance, mais aussi un gain dâ€™interprÃ©tabilitÃ©. En permettant de mieux comprendre les ressorts dÃ©cisionnels selon les groupes, elle contribue Ã  une modÃ©lisation plus transparente et maÃ®trisable, essentielle dans des contextes dâ€™aide Ã  la dÃ©cision.

Cette domination du prÃ©dicteur segmentÃ© chez les actifs soulÃ¨ve aussi une piste intÃ©ressante : le gain marginal apportÃ© par certains modÃ¨les de base Ã©tant trÃ¨s faible, il pourrait Ãªtre pertinent dâ€™explorer des versions simplifiÃ©es du mÃ©ta-modÃ¨le, voire des architectures hiÃ©rarchisÃ©es oÃ¹ seuls les prÃ©dicteurs les plus informatifs sont conservÃ©s. Une telle dÃ©marche permettrait de rÃ©duire la complexitÃ© sans perte significative de performance, tout en amÃ©liorant la stabilitÃ© et la rapiditÃ© des prÃ©dictions.

## Exemple d'analyse de faux positif â€” Actifs


```{python}
#| label: Exemple-analyse-des-faux-positifs
#| echo: false
#| warning: false
#| message: false

# Exemple dâ€™analyse des faux positifs : analyse par statut et catÃ©gorie de mÃ©tier

# ===============================================
# ParamÃ¨tres des seuils optimaux (F1-score)
# ===============================================
seuil_optimal_actifs = 0.424
seuil_optimal_retraites = 0.537


# ============================
# Faux positifs â€” ACTIFS
# ============================
proba_predites_actifs = model_actifs.predict_proba(X_stack_actifs)[:, 1]
prediction_binaire_actifs = (proba_predites_actifs >= seuil_optimal_actifs).astype(int)
faux_positifs_actifs = (y_actifs == 0) & (prediction_binaire_actifs == 1)

# Affichage du nombre de cas
nb_fp_actifs = faux_positifs_actifs.sum()
print(f"ğŸ” Nombre de faux positifs (actifs) : {nb_fp_actifs}")

# Reconstruction des index et sous-dataframes
df_train = pd.concat([X_stack_actifs, X_stack_retraites])
df_train["job_42_regroupe"] = df[["job_42_regroupe"]].loc[df_train.index]

indices_actifs = X_stack_actifs.index
indices_retraites = X_stack_retraites.index

# Extraction des individus concernÃ©s
faux_positifs_df_actifs = df_train.loc[indices_actifs[faux_positifs_actifs]]

# Visualisation par mÃ©tier
plt.figure(figsize=(10, 4))
sns.countplot(
    data=faux_positifs_df_actifs,
    y="job_42_regroupe",
    order=faux_positifs_df_actifs["job_42_regroupe"].value_counts().index
)
plt.title(f"Faux positifs par mÃ©tier â€” Actifs (n = {nb_fp_actifs})", color="navy")
plt.xlabel("Nombre de faux positifs")
plt.tight_layout()
plt.show()


# ================================
# Faux positifs â€” RETRAITÃ‰S
# ================================
proba_predites_retraites = model_retraites.predict_proba(X_stack_retraites)[:, 1]
prediction_binaire_retraites = (proba_predites_retraites >= seuil_optimal_retraites).astype(int)
faux_positifs_retraites = (y_retraites == 0) & (prediction_binaire_retraites == 1)

# Affichage du nombre de cas
nb_fp_retraites = faux_positifs_retraites.sum()
print(f"ğŸ” Nombre de faux positifs (retraitÃ©s) : {nb_fp_retraites}")

# Extraction des individus concernÃ©s
faux_positifs_df_retraites = df_train.loc[indices_retraites[faux_positifs_retraites]]

# Visualisation par mÃ©tier
plt.figure(figsize=(10, 4))
sns.countplot(
    data=faux_positifs_df_retraites,
    y="job_42_regroupe",
    order=faux_positifs_df_retraites["job_42_regroupe"].value_counts().index
)
plt.title(f"Faux positifs par mÃ©tier â€” RetraitÃ©s (n = {nb_fp_retraites})", color="navy")
plt.xlabel("Nombre de faux positifs")
plt.tight_layout()
plt.show()
```


Enfin, lâ€™analyse des faux positifs ouvre des pistes prÃ©cieuses pour le perfectionnement du systÃ¨me. Chez les actifs, les 2 889 faux positifs sont fortement concentrÃ©s dans certains profils professionnels : Ã©lÃ¨ves, Ã©tudiants, personnes sans activitÃ© professionnelle apparente ou mÃ©tiers administratifs intermÃ©diaires. Ces cas suggÃ¨rent soit une ambiguÃ¯tÃ© structurelle - par exemple des transitions non captÃ©es -, soit des limites dans les variables observÃ©es. Chez les retraitÃ©s, les 625 faux positifs se rÃ©partissent de maniÃ¨re similaire, notamment parmi les anciens employÃ©s et ouvriers, indiquant une continuitÃ© possible des biais de prÃ©diction entre gÃ©nÃ©rations professionnelles.

Le fait que ces erreurs se concentrent sur un nombre restreint de catÃ©gories invite Ã  des ajustements ciblÃ©s. Parmi les pistes envisagÃ©es : lâ€™enrichissement du jeu de donnÃ©es avec des variables de trajectoire ou de contexte, lâ€™intÃ©gration dâ€™informations temporelles sur les transitions professionnelles, ou encore lâ€™entraÃ®nement de modÃ¨les spÃ©cifiques sur les segments les plus ambigus. Ces amÃ©liorations pourraient permettre dâ€™affiner la granularitÃ© de la prÃ©diction, en traitant mieux les zones grises identifiÃ©es. 
Lâ€™exploration plus systÃ©matique dâ€™autres cas de faux positifs, au-delÃ  de lâ€™exemple illustrÃ© ici, permettrait Ã©galement dâ€™identifier des motifs rÃ©currents ou des facteurs de confusion non modÃ©lisÃ©s, enrichissant encore la comprÃ©hension fine du comportement du systÃ¨me dans ses zones de fragilitÃ©.

Au-delÃ , les analyses SHAP menÃ©es sur le mÃ©ta-modÃ¨le ont rÃ©vÃ©lÃ© une hÃ©tÃ©rogÃ©nÃ©itÃ© marquÃ©e dans les contributions locales des prÃ©dicteurs, tant entre les segments de population â€” actifs vs retraitÃ©s â€” quâ€™au sein mÃªme de chaque groupe. Ces dynamiques suggÃ¨rent que la performance du modÃ¨le repose non seulement sur la qualitÃ© des prÃ©dicteurs de base, mais aussi sur la capacitÃ© du systÃ¨me Ã  ajuster leur pondÃ©ration selon le profil de lâ€™individu.
Dans cette perspective, il pourrait Ãªtre intÃ©ressant dâ€™envisager un stacking adaptatif, dans lequel les poids attribuÃ©s aux prÃ©dicteurs de base ne seraient plus fixes, mais appris dynamiquement en fonction des caractÃ©ristiques dâ€™entrÃ©e. ConcrÃ¨tement, cela reviendrait Ã  remplacer le mÃ©ta-modÃ¨le par une fonction de pondÃ©ration paramÃ©trique. Cette derniÃ¨re, mise en Å“uvre, par exemple, par un petit rÃ©seau de neurones ou une combinaison linÃ©aire suivie dâ€™un softmax, prendrait en entrÃ©e les variables descriptives de lâ€™individu et produirait un ensemble de coefficients $\alpha_k(x)$, chacun associÃ© Ã  un modÃ¨le de base $k$.
Ces coefficients, normalisÃ©s pour former une distribution de poids, dÃ©termineraient alors la contribution de chaque prÃ©dicteur Ã  la prÃ©diction finale, de faÃ§on spÃ©cifique Ã  chaque individu. Autrement dit, au lieu dâ€™utiliser une combinaison fixe des modÃ¨les comme dans un stacking classique, le systÃ¨me ajusterait automatiquement les pondÃ©rations selon les profils, en donnant plus de poids aux modÃ¨les les plus pertinents dans un contexte donnÃ©.
Cette approche, analogue Ã  un mÃ©canisme dâ€™attention, permettrait au systÃ¨me de moduler localement sa stratÃ©gie de dÃ©cision en fonction des signaux les plus discriminants. Elle resterait compatible avec lâ€™interprÃ©tabilitÃ©, dans la mesure oÃ¹ les poids $\alpha_k(x)$ peuvent Ãªtre extraits, visualisÃ©s et analysÃ©s a posteriori pour comprendre quelles sources dâ€™information ont Ã©tÃ© privilÃ©giÃ©es et pourquoi.
Outre des bÃ©nÃ©fices attendus en termes de prÃ©cision, cette stratÃ©gie pourrait offrir une modÃ©lisation plus fine des comportements hÃ©tÃ©rogÃ¨nes, particuliÃ¨rement pertinente dans des contextes sociaux ou Ã©conomiques marquÃ©s par une diversitÃ© de profils et de logiques dÃ©cisionnelles, comme dans le cas Ã©tudiÃ© notamment. 


# Conclusion

Bien que certains modÃ¨les individuels, notamment le LightGBM global, aient affichÃ© dâ€™excellentes performances (F1-score Ã©levÃ©, AUC proche de 0.94, accuracy â‰ˆ 84â€¯% en test aveugle intermÃ©diaire), le choix final du mÃ©ta-modÃ¨le segmentÃ© et calibrÃ© ne repose pas uniquement sur une amÃ©lioration purement quantitative des scores. Il sâ€™inscrit Ã©galement dans une logique dâ€™Ã©quilibre global, tenant compte des spÃ©cificitÃ©s propres Ã  chacun des modÃ¨les, et visant Ã  maximiser Ã  la fois la robustesse, lâ€™adaptabilitÃ© et lâ€™explicabilitÃ© du systÃ¨me de prÃ©diction.

Le LightGBM seul aurait sans doute suffi dans une dÃ©marche exclusivement axÃ©e sur la performance brute. Toutefois, les analyses ont montrÃ© que la segmentation amÃ©liore non seulement les rÃ©sultats globaux (accuracy â‰ˆ 88â€“89â€¯%, AUC â‰ˆ 0.96), mais semble surtout permettre une meilleure gestion des cas atypiques, en particulier chez les actifs. LÃ  oÃ¹ des modÃ¨les globaux comme LightGBM ont tendance Ã  gÃ©nÃ©raliser uniformÃ©ment les dÃ©cisions, le mÃ©ta-modÃ¨le segmentÃ© module ses prÃ©dictions selon les spÃ©cificitÃ©s propres Ã  chaque sous-population.

Ce choix apparaÃ®t comme pertinent sur plusieurs plans :

- Statistiquement, le stacking segmentÃ© amÃ©liore lâ€™Ã©quilibre entre sensibilitÃ© et spÃ©cificitÃ©, grÃ¢ce Ã  une calibration fine, Ã  des seuils diffÃ©renciÃ©s, et Ã  une meilleure intÃ©gration des signaux issus de chaque sous-modÃ¨le ;
- MÃ©thodologiquement et opÃ©rationnellement, câ€™est surtout la structure segmentÃ©e â€” indÃ©pendamment du stacking â€” qui renforce lâ€™interprÃ©tabilitÃ© contextuelle et la modularitÃ© du systÃ¨me. Toutefois, certains rÃ©sultats ont mis en Ã©vidence quâ€™une segmentation seule, non accompagnÃ©e dâ€™un mÃ©canisme de rÃ©gulation ou dâ€™intÃ©gration, pouvait accentuer le risque de surapprentissage local, en particulier dans des sous-groupes trÃ¨s homogÃ¨nes ou sur-reprÃ©sentÃ©s.

Dans ce cadre, lâ€™ajout dâ€™un mÃ©ta-modÃ¨le dâ€™agrÃ©gation supervisÃ©e (stacking) apparaÃ®t comme un compromis robuste : il permet dâ€™exploiter les spÃ©cificitÃ©s de chaque segment tout en rÃ©gulant leur contribution relative Ã  la dÃ©cision finale. En somme, cette approche pragmatique vise un systÃ¨me capable de sâ€™adapter Ã  la diversitÃ© des profils comme Ã  lâ€™Ã©volution des donnÃ©es futures.

Au vu des rÃ©sultats obtenus en validation croisÃ©e et des prÃ©dictions intermÃ©diaires issues du LightGBM, il est raisonnable dâ€™espÃ©rer une accuracy lÃ©gÃ¨rement supÃ©rieure en test aveugle. Un tel progrÃ¨s viendrait non seulement confirmer les choix mÃ©thodologiques effectuÃ©s, mais aussi souligner lâ€™intÃ©rÃªt concret de la segmentation, de la calibration diffÃ©renciÃ©e et de lâ€™agrÃ©gation structurÃ©e dans une logique mÃ©tier.